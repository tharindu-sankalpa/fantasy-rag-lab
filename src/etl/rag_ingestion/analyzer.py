# Dependencies:
# pip install pandas structlog langchain-core

import pandas as pd
from typing import List, Optional, Dict, Any
from collections import Counter
from langchain_core.documents import Document
from src.utils.logger import logger

class ChunkAnalyzer:
    """
    Analyzes document chunks generated by the ingestion processor.
    
    Provides summary statistics and data export capabilities using structured logging.
    """

    def __init__(self, documents: List[Document]):
        """
        Initialize the analyzer with a list of documents.

        Args:
            documents: List of LangChain Document objects to analyze.
        """
        self.documents = documents
        self.log = logger.bind(component="chunk_analyzer")
        self.df = self._create_dataframe()

    def _create_dataframe(self) -> pd.DataFrame:
        """
        Converts documents to a Pandas DataFrame for easier analysis.
        
        Returns:
            pd.DataFrame: A DataFrame containing text, length, and metadata for each chunk.
        """
        data = []
        for doc in self.documents:
            row = {
                "text": doc.page_content,
                "length": len(doc.page_content),
                **doc.metadata
            }
            data.append(row)
        
        if not data:
            return pd.DataFrame()
            
        return pd.DataFrame(data)

    def log_summary(self) -> None:
        """
        Logs a summary report of the chunks.
        """
        if self.df.empty:
            self.log.warning("no_data_to_analyze")
            return

        total_chunks = len(self.df)
        avg_size = float(self.df["length"].mean())
        max_size = int(self.df["length"].max())
        min_size = int(self.df["length"].min())
        
        # Check for missing metadata
        na_chapters = 0
        if "chapter_title" in self.df.columns and "chapter_number" in self.df.columns:
            na_condition = (self.df["chapter_title"].astype(str) == "N/A") & \
                           (self.df["chapter_number"].astype(str) == "N/A")
            na_chapters = self.df[na_condition].shape[0]

        self.log.info("chunk_analysis_report", 
                      total_chunks=total_chunks,
                      avg_size=avg_size,
                      max_size=max_size,
                      min_size=min_size,
                      na_chapters=na_chapters,
                      na_percentage=round(na_chapters/total_chunks*100, 1))

        # Log Book Distribution
        if "book_name" in self.df.columns:
            book_counts = self.df["book_name"].value_counts().to_dict()
            self.log.info("book_distribution", distribution=book_counts)

            # Log unique chapters per book
            for book, count in book_counts.items():
                unique_chapters = 0
                if "chapter_number" in self.df.columns and "chapter_title" in self.df.columns:
                    book_mask = self.df["book_name"] == book
                    unique_chapters = self.df[book_mask][["chapter_number", "chapter_title"]].drop_duplicates().shape[0]
                
                self.log.info("book_detail", book=book, chunks=count, unique_chapters=unique_chapters)

        # Log a few samples
        for i, doc in enumerate(self.documents[:3]):
             preview = doc.page_content[:200].replace('\n', ' ')
             self.log.info("sample_chunk", index=i, metadata=doc.metadata, content_preview=preview)

    def save_data(self, output_path: str) -> None:
        """
        Saves the chunk data to a file. Supports .json, .csv, and .parquet.

        Args:
            output_path: Destination file path.
        """
        if self.df.empty:
            self.log.warning("no_data_to_save")
            return

        try:
            if output_path.endswith('.json'):
                self.df.to_json(output_path, orient='records', indent=2)
            elif output_path.endswith('.csv'):
                self.df.to_csv(output_path, index=False)
            elif output_path.endswith('.parquet'):
                self.df.to_parquet(output_path, index=False)
            else:
                self.df.to_json(output_path, orient='records', indent=2)
            
            self.log.info("analysis_saved", file=output_path, format=output_path.split('.')[-1])
            
        except Exception as e:
            self.log.error("analysis_save_failed", error=str(e))
