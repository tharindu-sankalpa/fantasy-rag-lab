# Dependencies:
# pip install pandas structlog langchain-core

import pandas as pd
from typing import List, Optional
from collections import Counter
from langchain_core.documents import Document
from src.utils.logger import logger

class ChunkAnalyzer:
    """
    Analyzes document chunks generated by the ingestion processor.
    Provides summary statistics and data export capabilities.
    """

    def __init__(self, documents: List[Document]):
        """
        Initialize the analyzer with a list of documents.

        Args:
            documents: List of LangChain Document objects to analyze.
        """
        self.documents = documents
        self.log = logger.bind(component="chunk_analyzer")
        self.df = self._create_dataframe()

    def _create_dataframe(self) -> pd.DataFrame:
        """
        Converts documents to a Pandas DataFrame for easier analysis.
        """
        data = []
        for doc in self.documents:
            row = {
                "text": doc.page_content,
                "length": len(doc.page_content),
                **doc.metadata
            }
            data.append(row)
        
        if not data:
            return pd.DataFrame()
            
        return pd.DataFrame(data)

    def print_summary(self) -> None:
        """
        Prints a text-based summary report of the chunks to stdout.
        """
        if self.df.empty:
            self.log.warning("no_data_to_analyze")
            return

        total_chunks = len(self.df)
        avg_size = self.df["length"].mean()
        max_size = self.df["length"].max()
        min_size = self.df["length"].min()
        
        # Check for missing metadata
        na_chapters = 0
        if "chapter_title" in self.df.columns and "chapter_number" in self.df.columns:
            na_condition = (self.df["chapter_title"].astype(str) == "N/A") & \
                           (self.df["chapter_number"].astype(str) == "N/A")
            na_chapters = self.df[na_condition].shape[0]

        print("\n" + "="*50)
        print(f"CHUNK ANALYSIS REPORT")
        print("="*50)
        print(f"Total Chunks: {total_chunks}")
        print(f"Avg Chunk Size: {avg_size:.2f} chars")
        print(f"Max Chunk Size: {max_size} chars")
        print(f"Min Chunk Size: {min_size} chars")
        print(f"Chunks with N/A Chapter Info: {na_chapters} ({na_chapters/total_chunks*100:.1f}%)")

        print("\n--- Book Distribution ---")
        if "book_name" in self.df.columns:
            book_counts = self.df["book_name"].value_counts()
            for book, count in book_counts.items():
                unique_chapters = 0
                if "chapter_number" in self.df.columns and "chapter_title" in self.df.columns:
                    # Create a composite key for unique chapters
                    book_mask = self.df["book_name"] == book
                    unique_chapters = self.df[book_mask][["chapter_number", "chapter_title"]].drop_duplicates().shape[0]
                
                print(f"  - {book}: {count} chunks, {unique_chapters} unique chapters detected")

        print("\n--- Sample Chunks (First 3) ---")
        for i, doc in enumerate(self.documents[:3]):
            print(f"\n[Chunk {i+1}]")
            print(f"Metadata: {doc.metadata}")
            preview = doc.page_content[:200].replace('\n', ' ')
            print(f"Content Preview (first 200 chars): {preview}...")

    def save_data(self, output_path: str) -> None:
        """
        Saves the chunk data to a file. Supports .json, .csv, and .parquet.

        Args:
            output_path: Destination file path.
        """
        if self.df.empty:
            self.log.warning("no_data_to_save")
            return

        try:
            if output_path.endswith('.json'):
                self.df.to_json(output_path, orient='records', indent=2)
            elif output_path.endswith('.csv'):
                self.df.to_csv(output_path, index=False)
            elif output_path.endswith('.parquet'):
                self.df.to_parquet(output_path, index=False)
            else:
                # Default to JSON if unknown
                self.df.to_json(output_path, orient='records', indent=2)
            
            self.log.info("analysis_saved", file=output_path, format=output_path.split('.')[-1])
            
        except Exception as e:
            self.log.error("analysis_save_failed", error=str(e))
